# -*- coding: utf-8 -*-
"""(GPU) Damilola Adebayo -  Submitted Lab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A_Z0Gf-x_VxraY3dVum-toKhreQjodVp

# **A. Linear Regression (using GPU)**
"""

# Import required libraries
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import time
begintime = time.time()

tf.config.list_physical_devices('GPU')

# Define the linear predictor
def prediction(x, w, b):
    return w * x + b

# Define different loss functions
def mse_loss(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

def mae_loss(y_true, y_pred):
    return tf.reduce_mean(tf.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    is_small_error = tf.abs(error) <= delta
    squared_loss = 0.5 * tf.square(error)
    linear_loss = delta * tf.abs(error) - 0.5 * tf.square(delta)
    return tf.reduce_mean(tf.where(is_small_error, squared_loss, linear_loss))

def hybrid_loss(y_true, y_pred, alpha=0.5):
    mse = mse_loss(y_true, y_pred)
    mae = mae_loss(y_true, y_pred)
    return alpha * mse + (1 - alpha) * mae

# Training loop function
def train_model(X, y, W, b, learning_rate, train_steps, loss_fn=[]): # The different losses are to be passed to the loss_fn array
  history = {}
  for i in loss_fn:
    if i==None: print(f"Specify a loss function")
    elif i: print(f"Training with {i.__name__}")

    key = f"loss_history_{i.__name__}"

    start_time = time.time()
    best_loss = float('inf')

    for step in range(train_steps):
        with tf.GradientTape() as tape:
            pred = prediction(X, W, b)
            loss = i(y, pred)

        gradients = tape.gradient(loss, [W, b])
        W.assign_sub(learning_rate * gradients[0])
        b.assign_sub(learning_rate * gradients[1])

        current_loss = loss.numpy()
        history.setdefault(key, []).append(loss.numpy())

        if current_loss < best_loss:
            best_loss = current_loss
            patience_counter = 0
        else:
            patience_counter += 1

        # If the loss hasn't improved for 'patience' steps, reduce the learning rate.
        if patience_counter >= patience:
            learning_rate *= lr_decay_factor
            print(f"Reducing learning rate to {learning_rate:.6f} at step {i}")
            patience_counter = 0  # Reset the counter after reducing LR

        if step % 100 == 0:
            print(f"Step {step}, Loss: {loss.numpy():.4f}")

    training_time = time.time() - start_time
    print(f"\nTraining completed in {training_time:.2f} seconds")
    print(f"{W, b, history[key]}")
    plot_results(X, y, W, b,  history[key], "Actual vs Predicted Linear Regression Results")

  # Compare all loss functions
  print(history)
  plt.figure(figsize=(10, 6))
  plt.plot(list(history.values())[0], label='MSE')
  plt.plot(list(history.values())[1], label='MAE')
  plt.plot(list(history.values())[2], label='Huber')
  plt.plot(list(history.values())[3], label='Hybrid')
  plt.xlabel('Step')
  plt.ylabel('Loss')
  plt.title('Comparison of Loss Functions')
  plt.legend()
  plt.grid(True)
  plt.show()



# Plot results
def plot_results(X, y, W, b, loss_history, title="Linear Regression Rts"):
    plt.figure(figsize=(12, 5))

    # Plot the data and prediction
    plt.subplot(1, 2, 1)
    plt.scatter(X.numpy(), y.numpy(), alpha=0.5, label='Original Data')
    plt.scatter(X.numpy(), prediction(X, W, b).numpy(), alpha=0.1, label='Predicted Data')

    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(title)
    plt.legend()
    plt.grid(True)

    # Plot the loss history
    plt.subplot(1, 2, 2)
    plt.plot(loss_history, "-")
    plt.xlabel('Step')
    plt.ylabel('Loss')
    plt.title('Training Loss Over Time')
    plt.grid(True)

    plt.tight_layout()
    plt.show()

"""## SEED"""

# Converting first name to decimal for seed
NAME = "Damilola"
SEED = sum(ord(c) for c in NAME)
tf.random.set_seed(SEED)
np.random.seed(SEED)

"""## Using random Normal (Guassian) Noise"""

print("Using Normal Noise")
NUM_EXAMPLES = 500
TRAIN_STEPS = 1000
LEARNING_RATE = 0.001
patience = 300            # Number of steps to wait before reducing LR
lr_decay_factor = 0.5

patience_counter = 0
# Generate data with noise
X = tf.random.normal([NUM_EXAMPLES])

# Initialize vweight and bias
W = tf.Variable(tf.random.normal([1]))
b = tf.Variable(tf.random.normal([1]))

noise = tf.random.normal([NUM_EXAMPLES], mean=0.0, stddev=0.1)
y = X * 3 + 2 + noise
train_model(X, y, W, b, LEARNING_RATE,
            TRAIN_STEPS,
            loss_fn=[mse_loss, mae_loss, huber_loss, hybrid_loss]
            )

"""## Using Laplacian noise"""

print("Using Guassian Noise")
NUM_EXAMPLES = 500
TRAIN_STEPS = 1000
LEARNING_RATE = 0.001
patience = 300            # Number of steps to wait before reducing LR
lr_decay_factor = 0.5
patience_counter = 0

# Generate data
X = tf.random.normal([NUM_EXAMPLES])

# Initialize vweight and bias
W = tf.Variable(tf.random.normal([1]))
b = tf.Variable(tf.random.normal([1]))


noise=np.random.laplace(loc=0.0, scale=0.05, size=([NUM_EXAMPLES]))
y = X * 3 + 2 + noise
train_model(X, y, W, b, LEARNING_RATE, TRAIN_STEPS,
            loss_fn=[mse_loss, mae_loss, huber_loss, hybrid_loss]
            )

"""## Change Learning Rate and using same level of Guassian Noise"""

print("Changing the Learning Rate")
NUM_EXAMPLES = 500
TRAIN_STEPS = 1000
LEARNING_RATE = 5
patience = 300            # Number of steps to wait before reducing LR
lr_decay_factor = 0.5
patience_counter = 0

# Generate data
X = tf.random.normal([NUM_EXAMPLES])

# Initialize vweight and bias
W = tf.Variable(tf.random.normal([1]))
b = tf.Variable(tf.random.normal([1]))

# Generate data with noise
noise = tf.random.normal([NUM_EXAMPLES], mean=0.0, stddev=0.1)
y = X * 3 + 2 + noise

train_model(X, y, W, b, LEARNING_RATE,
            TRAIN_STEPS,
            loss_fn=[mse_loss, mae_loss, huber_loss, hybrid_loss]
            )

"""## Change Learning Rate and using same level of Laplacian Noise"""

print("Changing the Learning Rate to 5")
NUM_EXAMPLES = 500
TRAIN_STEPS = 1000
LEARNING_RATE = 5
patience = 300            # Number of steps to wait before reducing LR
lr_decay_factor = 0.5
patience_counter = 0

# Generate data
X = tf.random.normal([NUM_EXAMPLES])

# Initialize vweight and bias
W = tf.Variable(tf.random.normal([1]))
b = tf.Variable(tf.random.normal([1]))

noise=np.random.laplace(loc=0.0, scale=0.05, size=([NUM_EXAMPLES]))
y = X * 3 + 2 + noise
train_model(X, y, W, b, LEARNING_RATE, TRAIN_STEPS,
            loss_fn=[mse_loss, mae_loss, huber_loss, hybrid_loss]
            )

"""## Using Learning rate = 1.0"""

print("Changing the Learning Rate to 5")
NUM_EXAMPLES = 500
TRAIN_STEPS = 1000
LEARNING_RATE = 1
patience = 300            # Number of steps to wait before reducing LR
lr_decay_factor = 0.5
patience_counter = 0

# Generate data
X = tf.random.normal([NUM_EXAMPLES])

# Initialize vweight and bias
W = tf.Variable(tf.random.normal([1]))
b = tf.Variable(tf.random.normal([1]))

noise=np.random.laplace(loc=0.0, scale=0.05, size=([NUM_EXAMPLES]))
y = X * 3 + 2 + noise
train_model(X, y, W, b, LEARNING_RATE, TRAIN_STEPS,
            loss_fn=[mse_loss, mae_loss, huber_loss, hybrid_loss]
            )

fullyfinishtime=time.time()

print("Total execution time"(fullyfinishtime-begintime))